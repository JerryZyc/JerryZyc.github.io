---
layout: single
title: "探索与利用 Exploration Exploitation"
permalink: /learning/reinforcement-learning/fundamentals/exploration-exploitation/
classes: wide
---

## 一句话定义
探索与利用平衡未知环境中的试探行为与已知高回报行为。

## 问题设定
- 输入：环境与策略更新规则。
- 输出：平衡策略。
- 假设：环境回报可估计。
- 边界：纯利用易陷局部最优，纯探索低效。

## 数学表述
$\epsilon$-greedy：
$$
\pi(a\mid s) = \begin{cases}
\text{random} & \text{with prob } \epsilon \\
\arg\max_a Q(s,a) & \text{otherwise}
\end{cases}
$$

## 算法解释
- 引入随机性避免陷入局部最优。

## 优化与实现细节
- 数值要点：$\epsilon$ 可随时间衰减。

## 关联与边界
- 与 bandit 问题相通。
- 边界：连续动作需噪声策略。

## 失败模式
- 探索不足导致策略停滞。
- 探索过多导致收敛慢。

## 最小伪代码
```text
With prob epsilon choose random action
Otherwise choose greedy action
```

## 决策清单
- [ ] 探索策略与任务匹配
- [ ] 探索强度随时间调整
- [ ] 评估探索收益

## 个人备注
TODO
